
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Evaluation &#8212; &lt;h1 style=&#34;font-size:2em;text-align:center;color:#FF5733&#34;&gt;Recommenders&lt;/h1&gt;</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/tabs.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title"><h1 style="font-size:2em;text-align:center;color:#FF5733">Recommenders</h1></h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p class="caption" role="heading">
 <span class="caption-text">
  Getting Started Notebooks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00_quick_start/als_movielens.html">
   Running ALS on MovieLens (PySpark)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../00_quick_start/ncf_movielens.html">
   Neural Collaborative Filtering on MovieLens dataset.
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/examples/03_evaluate/evaluation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/microsoft/genalog"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/microsoft/genalog/issues/new?title=Issue%20on%20page%20%2Fexamples/03_evaluate/evaluation.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/microsoft/genalog/main?urlpath=tree/docs/genalog_docs/examples/03_evaluate/evaluation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#global-settings">
   0 Global settings
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prepare-data">
   1 Prepare data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prepare-dummy-data">
     1.1 Prepare dummy data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prepare-spark-data">
     1.2 Prepare Spark data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation-metrics">
   2 Evaluation metrics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rating-metrics">
     2.1 Rating metrics
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#use-cases">
       2.1.1 Use cases
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-to-with-the-evaluation-utilities">
       2.1.2 How-to with the evaluation utilities
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#root-mean-square-error-rmse">
       2.1.3 Root Mean Square Error (RMSE)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#r-squared-r2">
       2.1.4 R Squared (R2)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#mean-absolute-error-mae">
       2.1.5 Mean Absolute Error (MAE)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#explained-variance">
       2.1.6 Explained Variance
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#summary">
       2.1.7 Summary
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ranking-metrics">
     2.2 Ranking metrics
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       2.2.1 Use cases
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-to-with-evaluation-utilities">
       2.2.2 How-to with evaluation utilities
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#relevancy-of-recommendation">
       2.2.1 Relevancy of recommendation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#precision">
       2.2.1 Precision
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#recall">
       2.2.2 Recall
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#normalized-discounted-cumulative-gain-ndcg">
       2.2.3 Normalized Discounted Cumulative Gain (NDCG)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#mean-average-precision-map">
       2.2.4 Mean Average Precision (MAP)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#roc-and-auc">
       2.2.5 ROC and AUC
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#logistic-loss">
       2.3.2 Logistic loss
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       2.2.5 Summary
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><i>Copyright (c) Microsoft Corporation. All rights reserved.</i></p>
<p><i>Licensed under the MIT License.</i></p>
<div class="tex2jax_ignore mathjax_ignore section" id="evaluation">
<h1>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h1>
<p>Evaluation with offline metrics is pivotal to assess the quality of a recommender before it goes into production. Usually, evaluation metrics are carefully chosen based on the actual application scenario of a recommendation system. It is hence important to data scientists and AI developers that build recommendation systems to understand how each evaluation metric is calculated and what it is for.</p>
<p>This notebook deep dives into several commonly used evaluation metrics, and illustrates how these metrics are used in practice. The metrics covered in this notebook are merely for off-line evaluations.</p>
<div class="section" id="global-settings">
<h2>0 Global settings<a class="headerlink" href="#global-settings" title="Permalink to this headline">¶</a></h2>
<p>Most of the functions used in the notebook can be found in the <code class="docutils literal notranslate"><span class="pre">recommenders</span></code> directory.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># set the environment path to find Recommenders</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">pyspark</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">minmax_scale</span>

<span class="kn">from</span> <span class="nn">recommenders.utils.spark_utils</span> <span class="kn">import</span> <span class="n">start_or_get_spark</span>
<span class="kn">from</span> <span class="nn">recommenders.evaluation.spark_evaluation</span> <span class="kn">import</span> <span class="n">SparkRankingEvaluation</span><span class="p">,</span> <span class="n">SparkRatingEvaluation</span>
<span class="kn">from</span> <span class="nn">recommenders.evaluation.python_evaluation</span> <span class="kn">import</span> <span class="n">auc</span><span class="p">,</span> <span class="n">logloss</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;System version: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">version</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Pandas version: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PySpark version: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pyspark</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>System version: 3.6.0 | packaged by conda-forge | (default, Feb  9 2017, 14:36:55) 
[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]
Pandas version: 0.23.4
PySpark version: 2.3.1
</pre></div>
</div>
</div>
</div>
<p>Note to successfully run Spark codes with the Jupyter kernel, one needs to correctly set the environment variables of <code class="docutils literal notranslate"><span class="pre">PYSPARK_PYTHON</span></code> and <code class="docutils literal notranslate"><span class="pre">PYSPARK_DRIVER_PYTHON</span></code> that point to Python executables with the desired version. Detailed information can be found in the setup instruction document <span class="xref myst">SETUP.md</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">COL_USER</span> <span class="o">=</span> <span class="s2">&quot;UserId&quot;</span>
<span class="n">COL_ITEM</span> <span class="o">=</span> <span class="s2">&quot;MovieId&quot;</span>
<span class="n">COL_RATING</span> <span class="o">=</span> <span class="s2">&quot;Rating&quot;</span>
<span class="n">COL_PREDICTION</span> <span class="o">=</span> <span class="s2">&quot;Rating&quot;</span>

<span class="n">HEADER</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;col_user&quot;</span><span class="p">:</span> <span class="n">COL_USER</span><span class="p">,</span>
    <span class="s2">&quot;col_item&quot;</span><span class="p">:</span> <span class="n">COL_ITEM</span><span class="p">,</span>
    <span class="s2">&quot;col_rating&quot;</span><span class="p">:</span> <span class="n">COL_RATING</span><span class="p">,</span>
    <span class="s2">&quot;col_prediction&quot;</span><span class="p">:</span> <span class="n">COL_PREDICTION</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="prepare-data">
<h2>1 Prepare data<a class="headerlink" href="#prepare-data" title="Permalink to this headline">¶</a></h2>
<div class="section" id="prepare-dummy-data">
<h3>1.1 Prepare dummy data<a class="headerlink" href="#prepare-dummy-data" title="Permalink to this headline">¶</a></h3>
<p>For illustration purpose, a dummy data set is created for demonstrating how different evaluation metrics work.</p>
<p>The data has the schema that can be frequently found in a recommendation problem, that is, each row in the dataset is a (user, item, rating) tuple, where “rating” can be an ordinal rating score (e.g., discrete integers of 1, 2, 3, etc.) or an numerical float number that quantitatively indicates the preference of the user towards that item.</p>
<p>For simplicity reason, the column of rating in the dummy dataset we use in the example represent some ordinal ratings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_true</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="n">COL_USER</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
            <span class="n">COL_ITEM</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">],</span>
            <span class="n">COL_RATING</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">}</span>
    <span class="p">)</span>
<span class="n">df_pred</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="n">COL_USER</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
        <span class="n">COL_ITEM</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">14</span><span class="p">],</span>
        <span class="n">COL_PREDICTION</span><span class="p">:</span> <span class="p">[</span><span class="mi">14</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
    <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Take a look at ratings of the user with ID “1” in the dummy dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_true</span><span class="p">[</span><span class="n">df_true</span><span class="p">[</span><span class="n">COL_USER</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>UserId</th>
      <th>MovieId</th>
      <th>Rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>2</td>
      <td>4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_pred</span><span class="p">[</span><span class="n">df_pred</span><span class="p">[</span><span class="n">COL_USER</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>UserId</th>
      <th>MovieId</th>
      <th>Rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>3</td>
      <td>14</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>10</td>
      <td>13</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>12</td>
      <td>12</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="prepare-spark-data">
<h3>1.2 Prepare Spark data<a class="headerlink" href="#prepare-spark-data" title="Permalink to this headline">¶</a></h3>
<p>Spark framework is sometimes used to evaluate metrics given datasets that are hard to fit into memory. In our example, Spark DataFrames can be created from the Python dummy dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span> <span class="o">=</span> <span class="n">start_or_get_spark</span><span class="p">(</span><span class="s2">&quot;EvaluationTesting&quot;</span><span class="p">,</span> <span class="s2">&quot;local&quot;</span><span class="p">)</span>

<span class="n">dfs_true</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">df_true</span><span class="p">)</span>
<span class="n">dfs_pred</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">df_pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dfs_true</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">dfs_true</span><span class="p">[</span><span class="n">COL_USER</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+------+-------+------+
|UserId|MovieId|Rating|
+------+-------+------+
|     1|      1|     5|
|     1|      2|     4|
|     1|      3|     3|
+------+-------+------+
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dfs_pred</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">dfs_pred</span><span class="p">[</span><span class="n">COL_USER</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+------+-------+------+
|UserId|MovieId|Rating|
+------+-------+------+
|     1|      3|    14|
|     1|     10|    13|
|     1|     12|    12|
+------+-------+------+
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="evaluation-metrics">
<h2>2 Evaluation metrics<a class="headerlink" href="#evaluation-metrics" title="Permalink to this headline">¶</a></h2>
<div class="section" id="rating-metrics">
<h3>2.1 Rating metrics<a class="headerlink" href="#rating-metrics" title="Permalink to this headline">¶</a></h3>
<p>Rating metrics are similar to regression metrics used for evaluating a regression model that predicts numerical values given input observations. In the context of recommendation system, rating metrics are to evaluate how accurate a recommender is to predict ratings that users may give to items. Therefore, the metrics are <strong>calculated exactly on the same group of (user, item) pairs that exist in both ground-truth dataset and prediction dataset</strong> and <strong>averaged by the total number of users</strong>.</p>
<div class="section" id="use-cases">
<h4>2.1.1 Use cases<a class="headerlink" href="#use-cases" title="Permalink to this headline">¶</a></h4>
<p>Rating metrics are effective in measuring the model accuracy. However, in some cases, the rating metrics are limited if</p>
<ul class="simple">
<li><p><strong>the recommender is to predict ranking instead of explicit rating</strong>. For example, if the consumer of the recommender cares about the ranked recommended items, rating metrics do not apply directly. Usually a relevancy function such as top-k will be applied to generate the ranked list from predicted ratings in order to evaluate the recommender with other metrics.</p></li>
<li><p><strong>the recommender is to generate recommendation scores that have different scales with the original ratings (e.g., the SAR algorithm)</strong>. In this case, the difference between the generated scores and the original scores (or, ratings) is not valid for measuring accuracy of the model.</p></li>
</ul>
</div>
<div class="section" id="how-to-with-the-evaluation-utilities">
<h4>2.1.2 How-to with the evaluation utilities<a class="headerlink" href="#how-to-with-the-evaluation-utilities" title="Permalink to this headline">¶</a></h4>
<p>A few notes about the interface of the Rating evaluator class:</p>
<ol class="simple">
<li><p>The columns of user, item, and rating (prediction) should be present in the ground-truth DataFrame (prediction DataFrame).</p></li>
<li><p>There should be no duplicates of (user, item) pairs in the ground-truth and the prediction DataFrames, othewise there may be unexpected behavior in calculating certain metrics.</p></li>
<li><p>Default column names for user, item, rating, and prediction are “UserId”, “ItemId”, “Rating”, and “Prediciton”, respectively.</p></li>
</ol>
<p>In our examples below, to calculate rating metrics for input data frames in Spark, a Spark object, <code class="docutils literal notranslate"><span class="pre">SparkRatingEvaluation</span></code> is initialized. The input data schemas for the ground-truth dataset and the prediction dataset are</p>
<ul class="simple">
<li><p>Ground-truth dataset.</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Column</p></th>
<th class="head"><p>Data type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">COL_USER</span></code></p></td>
<td><p>&lt;int&gt;</p></td>
<td><p>User ID</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">COL_ITEM</span></code></p></td>
<td><p>&lt;int&gt;</p></td>
<td><p>Item ID</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">COL_RATING</span></code></p></td>
<td><p>&lt;float&gt;</p></td>
<td><p>Rating or numerical value of user preference.</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>Prediction dataset.</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Column</p></th>
<th class="head"><p>Data type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">COL_USER</span></code></p></td>
<td><p>&lt;int&gt;</p></td>
<td><p>User ID</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">COL_ITEM</span></code></p></td>
<td><p>&lt;int&gt;</p></td>
<td><p>Item ID</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">COL_RATING</span></code></p></td>
<td><p>&lt;float&gt;</p></td>
<td><p>Predicted rating or numerical value of user preference.</p></td>
</tr>
</tbody>
</table>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spark_rate_eval</span> <span class="o">=</span> <span class="n">SparkRatingEvaluation</span><span class="p">(</span><span class="n">dfs_true</span><span class="p">,</span> <span class="n">dfs_pred</span><span class="p">,</span> <span class="o">**</span><span class="n">HEADER</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="root-mean-square-error-rmse">
<h4>2.1.3 Root Mean Square Error (RMSE)<a class="headerlink" href="#root-mean-square-error-rmse" title="Permalink to this headline">¶</a></h4>
<p>RMSE is for evaluating the accuracy of prediction on ratings. RMSE is the most widely used metric to evaluate a recommendation algorithm that predicts missing ratings. The benefit is that RMSE is easy to explain and calculate.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The RMSE is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">spark_rate_eval</span><span class="o">.</span><span class="n">rmse</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The RMSE is 7.254309064273455
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="r-squared-r2">
<h4>2.1.4 R Squared (R2)<a class="headerlink" href="#r-squared-r2" title="Permalink to this headline">¶</a></h4>
<p>R2 is also called “coefficient of determination” in some context. It is a metric that evaluates how well a regression model performs, based on the proportion of total variations of the observed results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The R2 is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">spark_rate_eval</span><span class="o">.</span><span class="n">rsquared</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The R2 is -31.699029126213595
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="mean-absolute-error-mae">
<h4>2.1.5 Mean Absolute Error (MAE)<a class="headerlink" href="#mean-absolute-error-mae" title="Permalink to this headline">¶</a></h4>
<p>MAE evaluates accuracy of prediction. It computes the metric value from ground truths and prediction in the same scale. Compared to RMSE, MAE is more explainable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The MAE is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">spark_rate_eval</span><span class="o">.</span><span class="n">mae</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The MAE is 6.375
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="explained-variance">
<h4>2.1.6 Explained Variance<a class="headerlink" href="#explained-variance" title="Permalink to this headline">¶</a></h4>
<p>Explained variance is usually used to measure how well a model performs with regard to the impact from the variation of the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The explained variance is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">spark_rate_eval</span><span class="o">.</span><span class="n">exp_var</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The explained variance is -6.446601941747574
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="summary">
<h4>2.1.7 Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h4>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Range</p></th>
<th class="head"><p>Selection criteria</p></th>
<th class="head"><p>Limitation</p></th>
<th class="head"><p>Reference</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>RMSE</p></td>
<td><p><span class="math notranslate nohighlight">\(&gt; 0\)</span></p></td>
<td><p>The smaller the better.</p></td>
<td><p>May be biased, and less explainable than MSE</p></td>
<td><p><a class="reference external" href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">link</a></p></td>
</tr>
<tr class="row-odd"><td><p>R2</p></td>
<td><p><span class="math notranslate nohighlight">\(\leq 1\)</span></p></td>
<td><p>The closer to <span class="math notranslate nohighlight">\(1\)</span> the better.</p></td>
<td><p>Depend on variable distributions.</p></td>
<td><p><a class="reference external" href="https://en.wikipedia.org/wiki/Coefficient_of_determination">link</a></p></td>
</tr>
<tr class="row-even"><td><p>MAE</p></td>
<td><p><span class="math notranslate nohighlight">\(\geq 0\)</span></p></td>
<td><p>The smaller the better.</p></td>
<td><p>Dependent on variable scale.</p></td>
<td><p><a class="reference external" href="https://en.wikipedia.org/wiki/Mean_absolute_error">link</a></p></td>
</tr>
<tr class="row-odd"><td><p>Explained variance</p></td>
<td><p><span class="math notranslate nohighlight">\(\leq 1\)</span></p></td>
<td><p>The closer to <span class="math notranslate nohighlight">\(1\)</span> the better.</p></td>
<td><p>Depend on variable distributions.</p></td>
<td><p><a class="reference external" href="https://en.wikipedia.org/wiki/Explained_variation">link</a></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="ranking-metrics">
<h3>2.2 Ranking metrics<a class="headerlink" href="#ranking-metrics" title="Permalink to this headline">¶</a></h3>
<p>“Beyond-accuray evaluation” was proposed to evaluate how relevant recommendations are for users. In this case, a recommendation system is a treated as a ranking system. Given a relency definition, recommendation system outputs a list of recommended items to each user, which is ordered by relevance. The evaluation part takes ground-truth data, the actual items that users interact with (e.g., liked, purchased, etc.), and the recommendation data, as inputs, to calculate ranking evaluation metrics.</p>
<div class="section" id="id1">
<h4>2.2.1 Use cases<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>Ranking metrics are often used when hit and/or ranking of the items are considered:</p>
<ul class="simple">
<li><p><strong>Hit</strong> - defined by relevancy, a hit usually means whether the recommended “k” items hit the “relevant” items by the user. For example, a user may have clicked, viewed, or purchased an item for many times, and a hit in the recommended items indicate that the recommender performs well. Metrics like “precision”, “recall”, etc. measure the performance of such hitting accuracy.</p></li>
<li><p><strong>Ranking</strong> - ranking metrics give more explanations about, for the hitted items, whether they are ranked in a way that is preferred by the users whom the items will be recommended to. Metrics like “mean average precision”, “ndcg”, etc., evaluate whether the relevant items are ranked higher than the less-relevant or irrelevant items.</p></li>
</ul>
</div>
<div class="section" id="how-to-with-evaluation-utilities">
<h4>2.2.2 How-to with evaluation utilities<a class="headerlink" href="#how-to-with-evaluation-utilities" title="Permalink to this headline">¶</a></h4>
<p>A few notes about the interface of the Rating evaluator class:</p>
<ol class="simple">
<li><p>The columns of user, item, and rating (prediction) should be present in the ground-truth DataFrame (prediction DataFrame). The column of timestamp is optional, but it is required if certain relevant function is used. For example, timestamps will be used if the most recent items are defined as the relevant one.</p></li>
<li><p>There should be no duplicates of (user, item) pairs in the ground-truth and the prediction DataFrames, othewise there may be unexpected behavior in calculating certain metrics.</p></li>
<li><p>Default column names for user, item, rating, and prediction are “UserId”, “ItemId”, “Rating”, and “Prediciton”, respectively.</p></li>
</ol>
</div>
<div class="section" id="relevancy-of-recommendation">
<h4>2.2.1 Relevancy of recommendation<a class="headerlink" href="#relevancy-of-recommendation" title="Permalink to this headline">¶</a></h4>
<p>Relevancy of recommendation can be measured in different ways:</p>
<ul class="simple">
<li><p><strong>By ranking</strong> - In this case, relevant items in the recommendations are defined as the top ranked items, i.e., top k items, which are taken from the list of the recommended items that is ordered by the predicted ratings (or other numerical scores that indicate preference of a user to an item).</p></li>
<li><p><strong>By timestamp</strong> - Relevant items are defined as the most recently viewed k items, which are obtained from the recommended items ranked by timestamps.</p></li>
<li><p><strong>By rating</strong> - Relevant items are defined as items with ratings (or other numerical scores that indicate preference of a user to an item) that are above a given threshold.</p></li>
</ul>
<p>Similarly, a ranking metric object can be initialized as below. The input data schema is</p>
<ul class="simple">
<li><p>Ground-truth dataset.</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Column</p></th>
<th class="head"><p>Data type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">COL_USER</span></code></p></td>
<td><p>&lt;int&gt;</p></td>
<td><p>User ID</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">COL_ITEM</span></code></p></td>
<td><p>&lt;int&gt;</p></td>
<td><p>Item ID</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">COL_RATING</span></code></p></td>
<td><p>&lt;float&gt;</p></td>
<td><p>Rating or numerical value of user preference.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">COL_TIMESTAMP</span></code></p></td>
<td><p>&lt;string&gt;</p></td>
<td><p>Timestamps.</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>Prediction dataset.</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Column</p></th>
<th class="head"><p>Data type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">COL_USER</span></code></p></td>
<td><p>&lt;int&gt;</p></td>
<td><p>User ID</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">COL_ITEM</span></code></p></td>
<td><p>&lt;int&gt;</p></td>
<td><p>Item ID</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">COL_RATING</span></code></p></td>
<td><p>&lt;float&gt;</p></td>
<td><p>Predicted rating or numerical value of user preference.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">COL_TIMESTAM</span></code></p></td>
<td><p>&lt;string&gt;</p></td>
<td><p>Timestamps.</p></td>
</tr>
</tbody>
</table>
<p>In this case, in addition to the input datasets, there are also other arguments used for calculating the ranking metrics:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Argument</p></th>
<th class="head"><p>Data type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">k</span></code></p></td>
<td><p>&lt;int&gt;</p></td>
<td><p>Number of items recommended to user.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">revelancy_method</span></code></p></td>
<td><p>&lt;string&gt;</p></td>
<td><p>Methonds that extract relevant items from the recommendation list</p></td>
</tr>
</tbody>
</table>
<p>For example, the following code initializes a ranking metric object that calculates the metrics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spark_rank_eval</span> <span class="o">=</span> <span class="n">SparkRankingEvaluation</span><span class="p">(</span><span class="n">dfs_true</span><span class="p">,</span> <span class="n">dfs_pred</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">relevancy_method</span><span class="o">=</span><span class="s2">&quot;top_k&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">HEADER</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>A few ranking metrics can then be calculated.</p>
</div>
<div class="section" id="precision">
<h4>2.2.1 Precision<a class="headerlink" href="#precision" title="Permalink to this headline">¶</a></h4>
<p>Precision&#64;k is a metric that evaluates how many items in the recommendation list are relevant (hit) in the ground-truth data. For each user the precision score is normalized by <code class="docutils literal notranslate"><span class="pre">k</span></code> and then the overall precision scores are averaged by the total number of users.</p>
<p>Note it is apparent that the precision&#64;k metric grows with the number of <code class="docutils literal notranslate"><span class="pre">k</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The precision at k is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">spark_rank_eval</span><span class="o">.</span><span class="n">precision_at_k</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The precision at k is 0.3333333333333333
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="recall">
<h4>2.2.2 Recall<a class="headerlink" href="#recall" title="Permalink to this headline">¶</a></h4>
<p>Recall&#64;k is a metric that evaluates how many relevant items in the ground-truth data are in the recommendation list. For each user the recall score is normalized by the total number of ground-truth items and then the overall recall scores are averaged by the total number of users.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The recall at k is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">spark_rank_eval</span><span class="o">.</span><span class="n">recall_at_k</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The recall at k is 0.2111111111111111
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="normalized-discounted-cumulative-gain-ndcg">
<h4>2.2.3 Normalized Discounted Cumulative Gain (NDCG)<a class="headerlink" href="#normalized-discounted-cumulative-gain-ndcg" title="Permalink to this headline">¶</a></h4>
<p>NDCG is a metric that evaluates how well the recommender performs in recommending ranked items to users. Therefore both hit of relevant items and correctness in ranking of these items matter to the NDCG evaluation. The total NDCG score is normalized by the total number of users.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The ndcg at k is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">spark_rank_eval</span><span class="o">.</span><span class="n">ndcg_at_k</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The ndcg at k is 0.3333333333333333
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="mean-average-precision-map">
<h4>2.2.4 Mean Average Precision (MAP)<a class="headerlink" href="#mean-average-precision-map" title="Permalink to this headline">¶</a></h4>
<p>MAP is a metric that evaluates the average precision for each user in the datasets. It also penalizes ranking correctness of the recommended items. The overall MAP score is normalized by the total number of users.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The map at k is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">spark_rank_eval</span><span class="o">.</span><span class="n">map_at_k</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The map at k is 0.15
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="roc-and-auc">
<h4>2.2.5 ROC and AUC<a class="headerlink" href="#roc-and-auc" title="Permalink to this headline">¶</a></h4>
<p>ROC, as well as AUC, is a well known metric that is used for evaluating binary classification problem. It is similar in the case of binary rating typed recommendation algorithm where the “hit” accuracy on the relevant items is used for measuring the recommender’s performance.</p>
<p>To demonstrate the evaluation method, the original data for testing is manipuldated in a way that the ratings in the testing data are arranged as binary scores, whilst the ones in the prediction are scaled in 0 to 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert the original rating to 0 and 1.</span>
<span class="n">df_true_bin</span> <span class="o">=</span> <span class="n">df_true</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">df_true_bin</span><span class="p">[</span><span class="n">COL_RATING</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_true_bin</span><span class="p">[</span><span class="n">COL_RATING</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">3</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">df_true_bin</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>UserId</th>
      <th>MovieId</th>
      <th>Rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2</td>
      <td>5</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2</td>
      <td>6</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2</td>
      <td>7</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>3</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>3</td>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>3</td>
      <td>6</td>
      <td>1</td>
    </tr>
    <tr>
      <th>11</th>
      <td>3</td>
      <td>8</td>
      <td>1</td>
    </tr>
    <tr>
      <th>12</th>
      <td>3</td>
      <td>9</td>
      <td>1</td>
    </tr>
    <tr>
      <th>13</th>
      <td>3</td>
      <td>10</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>3</td>
      <td>11</td>
      <td>0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>3</td>
      <td>12</td>
      <td>0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>3</td>
      <td>13</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>3</td>
      <td>14</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert the predicted ratings into a [0, 1] scale.</span>
<span class="n">df_pred_bin</span> <span class="o">=</span> <span class="n">df_pred</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">df_pred_bin</span><span class="p">[</span><span class="n">COL_PREDICTION</span><span class="p">]</span> <span class="o">=</span> <span class="n">minmax_scale</span><span class="p">(</span><span class="n">df_pred_bin</span><span class="p">[</span><span class="n">COL_PREDICTION</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">))</span>

<span class="n">df_pred_bin</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>UserId</th>
      <th>MovieId</th>
      <th>Rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>3</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>10</td>
      <td>0.888889</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>12</td>
      <td>0.777778</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>10</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>3</td>
      <td>0.888889</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2</td>
      <td>5</td>
      <td>0.777778</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2</td>
      <td>11</td>
      <td>0.666667</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2</td>
      <td>13</td>
      <td>0.555556</td>
    </tr>
    <tr>
      <th>8</th>
      <td>3</td>
      <td>4</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>9</th>
      <td>3</td>
      <td>10</td>
      <td>0.888889</td>
    </tr>
    <tr>
      <th>10</th>
      <td>3</td>
      <td>7</td>
      <td>0.777778</td>
    </tr>
    <tr>
      <th>11</th>
      <td>3</td>
      <td>13</td>
      <td>0.666667</td>
    </tr>
    <tr>
      <th>12</th>
      <td>3</td>
      <td>1</td>
      <td>0.555556</td>
    </tr>
    <tr>
      <th>13</th>
      <td>3</td>
      <td>3</td>
      <td>0.444444</td>
    </tr>
    <tr>
      <th>14</th>
      <td>3</td>
      <td>5</td>
      <td>0.333333</td>
    </tr>
    <tr>
      <th>15</th>
      <td>3</td>
      <td>2</td>
      <td>0.222222</td>
    </tr>
    <tr>
      <th>16</th>
      <td>3</td>
      <td>11</td>
      <td>0.111111</td>
    </tr>
    <tr>
      <th>17</th>
      <td>3</td>
      <td>14</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the AUC metric</span>
<span class="n">auc_score</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span>
    <span class="n">df_true_bin</span><span class="p">,</span>
    <span class="n">df_pred_bin</span><span class="p">,</span>
    <span class="n">col_user</span> <span class="o">=</span> <span class="n">COL_USER</span><span class="p">,</span>
    <span class="n">col_item</span> <span class="o">=</span> <span class="n">COL_ITEM</span><span class="p">,</span>
    <span class="n">col_rating</span> <span class="o">=</span> <span class="n">COL_RATING</span><span class="p">,</span>
    <span class="n">col_prediction</span> <span class="o">=</span> <span class="n">COL_RATING</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The auc score is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">auc_score</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The auc score is 0.3333
</pre></div>
</div>
</div>
</div>
<p>It is worth mentioning that in some literature there are variants of the original AUC metric, that considers the effect of <strong>the number of the recommended items (k)</strong>, <strong>grouping effect of users (compute AUC for each user group, and take the average across different groups)</strong>. These variants are applicable to various different scenarios, and choosing an appropriate one depends on the context of the use case itself.</p>
</div>
<div class="section" id="logistic-loss">
<h4>2.3.2 Logistic loss<a class="headerlink" href="#logistic-loss" title="Permalink to this headline">¶</a></h4>
<p>Logistic loss (sometimes it is called simply logloss, or cross-entropy loss) is another useful metric to evaluate the hit accuracy. It is defined as the negative log-likelihood of the true labels given the predictions of a classifier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the logloss metric</span>
<span class="n">logloss_score</span> <span class="o">=</span> <span class="n">logloss</span><span class="p">(</span>
    <span class="n">df_true_bin</span><span class="p">,</span>
    <span class="n">df_pred_bin</span><span class="p">,</span>
    <span class="n">col_user</span> <span class="o">=</span> <span class="n">COL_USER</span><span class="p">,</span>
    <span class="n">col_item</span> <span class="o">=</span> <span class="n">COL_ITEM</span><span class="p">,</span>
    <span class="n">col_rating</span> <span class="o">=</span> <span class="n">COL_RATING</span><span class="p">,</span>
    <span class="n">col_prediction</span> <span class="o">=</span> <span class="n">COL_RATING</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The logloss score is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logloss_score</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The logloss score is 4.1061
</pre></div>
</div>
</div>
</div>
<p>It is worth noting that logloss may be sensitive to the class balance of datasets, as it penalizes heavily classifiers that are confident about incorrect classifications. To demonstrate, the ground truth data set for testing is manipulated purposely to unbalance the binary labels. For example, the following binarizes the original rating data by using a lower threshold, i.e., 2, to create more positive feedback from the user.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_true_bin_pos</span> <span class="o">=</span> <span class="n">df_true</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">df_true_bin_pos</span><span class="p">[</span><span class="n">COL_RATING</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_true_bin_pos</span><span class="p">[</span><span class="n">COL_RATING</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">df_true_bin_pos</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>UserId</th>
      <th>MovieId</th>
      <th>Rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2</td>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2</td>
      <td>6</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2</td>
      <td>7</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>3</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>3</td>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>3</td>
      <td>6</td>
      <td>1</td>
    </tr>
    <tr>
      <th>11</th>
      <td>3</td>
      <td>8</td>
      <td>1</td>
    </tr>
    <tr>
      <th>12</th>
      <td>3</td>
      <td>9</td>
      <td>1</td>
    </tr>
    <tr>
      <th>13</th>
      <td>3</td>
      <td>10</td>
      <td>1</td>
    </tr>
    <tr>
      <th>14</th>
      <td>3</td>
      <td>11</td>
      <td>1</td>
    </tr>
    <tr>
      <th>15</th>
      <td>3</td>
      <td>12</td>
      <td>1</td>
    </tr>
    <tr>
      <th>16</th>
      <td>3</td>
      <td>13</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>3</td>
      <td>14</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>By using threshold of 2, the labels in the ground truth data is not balanced, and the ratio of 1 over 0 is</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">one_zero_ratio</span> <span class="o">=</span> <span class="n">df_true_bin_pos</span><span class="p">[</span><span class="n">COL_PREDICTION</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">df_true_bin_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">df_true_bin_pos</span><span class="p">[</span><span class="n">COL_PREDICTION</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The ratio between label 1 and label 0 is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">one_zero_ratio</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The ratio between label 1 and label 0 is 5.0
</pre></div>
</div>
</div>
</div>
<p>Another prediction data is also created, where the probabilities for label 1 and label 0 are fixed. Without loss of generity, the probability of predicting 1 is 0.6. The data set is purposely created to make the precision to be 100% given an presumption of cut-off equal to 0.5.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prob_true</span> <span class="o">=</span> <span class="mf">0.6</span>

<span class="n">df_pred_bin_pos</span> <span class="o">=</span> <span class="n">df_true_bin_pos</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">df_pred_bin_pos</span><span class="p">[</span><span class="n">COL_PREDICTION</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_pred_bin_pos</span><span class="p">[</span><span class="n">COL_PREDICTION</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">prob_true</span> <span class="k">if</span> <span class="n">x</span><span class="o">==</span><span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span><span class="o">-</span><span class="n">prob_true</span><span class="p">)</span>

<span class="n">df_pred_bin_pos</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>UserId</th>
      <th>MovieId</th>
      <th>Rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>2</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>1</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>4</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2</td>
      <td>5</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2</td>
      <td>6</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2</td>
      <td>7</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>8</th>
      <td>3</td>
      <td>2</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>9</th>
      <td>3</td>
      <td>5</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>10</th>
      <td>3</td>
      <td>6</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>11</th>
      <td>3</td>
      <td>8</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>12</th>
      <td>3</td>
      <td>9</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>13</th>
      <td>3</td>
      <td>10</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>14</th>
      <td>3</td>
      <td>11</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>15</th>
      <td>3</td>
      <td>12</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>16</th>
      <td>3</td>
      <td>13</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>17</th>
      <td>3</td>
      <td>14</td>
      <td>0.4</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Then the logloss is calculated as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the logloss metric</span>
<span class="n">logloss_score_pos</span> <span class="o">=</span> <span class="n">logloss</span><span class="p">(</span>
    <span class="n">df_true_bin_pos</span><span class="p">,</span>
    <span class="n">df_pred_bin_pos</span><span class="p">,</span>
    <span class="n">col_user</span> <span class="o">=</span> <span class="n">COL_USER</span><span class="p">,</span>
    <span class="n">col_item</span> <span class="o">=</span> <span class="n">COL_ITEM</span><span class="p">,</span>
    <span class="n">col_rating</span> <span class="o">=</span> <span class="n">COL_RATING</span><span class="p">,</span>
    <span class="n">col_prediction</span> <span class="o">=</span> <span class="n">COL_RATING</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The logloss score is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logloss_score</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The logloss score is 4.1061
</pre></div>
</div>
</div>
</div>
<p>For comparison, a similar process is used with a threshold value of 3 to create a more balanced dataset. Another prediction dataset is also created by using the balanced dataset. Again, the probabilities of predicting label 1 and label 0 are fixed as 0.6 and 0.4, respectively. <strong>NOTE</strong>, same as above, in this case, the prediction also gives us a 100% precision. The only difference is the proportion of binary labels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prob_true</span> <span class="o">=</span> <span class="mf">0.6</span>

<span class="n">df_pred_bin_balanced</span> <span class="o">=</span> <span class="n">df_true_bin</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">df_pred_bin_balanced</span><span class="p">[</span><span class="n">COL_PREDICTION</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_pred_bin_balanced</span><span class="p">[</span><span class="n">COL_PREDICTION</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">prob_true</span> <span class="k">if</span> <span class="n">x</span><span class="o">==</span><span class="mi">1</span> <span class="k">else</span> <span class="mi">1</span><span class="o">-</span><span class="n">prob_true</span><span class="p">)</span>

<span class="n">df_pred_bin_balanced</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>UserId</th>
      <th>MovieId</th>
      <th>Rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>2</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>1</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>4</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2</td>
      <td>5</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2</td>
      <td>6</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2</td>
      <td>7</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>8</th>
      <td>3</td>
      <td>2</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>9</th>
      <td>3</td>
      <td>5</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>10</th>
      <td>3</td>
      <td>6</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>11</th>
      <td>3</td>
      <td>8</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>12</th>
      <td>3</td>
      <td>9</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>13</th>
      <td>3</td>
      <td>10</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>14</th>
      <td>3</td>
      <td>11</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>15</th>
      <td>3</td>
      <td>12</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>16</th>
      <td>3</td>
      <td>13</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>17</th>
      <td>3</td>
      <td>14</td>
      <td>0.4</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The ratio of label 1 and label 0 is</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">one_zero_ratio</span> <span class="o">=</span> <span class="n">df_true_bin</span><span class="p">[</span><span class="n">COL_PREDICTION</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">df_true_bin</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">df_true_bin</span><span class="p">[</span><span class="n">COL_PREDICTION</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The ratio between label 1 and label 0 is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">one_zero_ratio</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The ratio between label 1 and label 0 is 1.0
</pre></div>
</div>
</div>
</div>
<p>It is perfectly balanced.</p>
<p>Applying the logloss function to calculate the metric gives us a more promising result, as shown below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the logloss metric</span>
<span class="n">logloss_score</span> <span class="o">=</span> <span class="n">logloss</span><span class="p">(</span>
    <span class="n">df_true_bin</span><span class="p">,</span>
    <span class="n">df_pred_bin_balanced</span><span class="p">,</span>
    <span class="n">col_user</span> <span class="o">=</span> <span class="n">COL_USER</span><span class="p">,</span>
    <span class="n">col_item</span> <span class="o">=</span> <span class="n">COL_ITEM</span><span class="p">,</span>
    <span class="n">col_rating</span> <span class="o">=</span> <span class="n">COL_RATING</span><span class="p">,</span>
    <span class="n">col_prediction</span> <span class="o">=</span> <span class="n">COL_RATING</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The logloss score is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logloss_score</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The logloss score is 0.5108
</pre></div>
</div>
</div>
</div>
<p>It can be seen that the score is more close to 0, and, by definition, it means that the predictions are generating better results than the one before where binary labels are more biased.</p>
</div>
<div class="section" id="id2">
<h4>2.2.5 Summary<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Range</p></th>
<th class="head"><p>Selection criteria</p></th>
<th class="head"><p>Limitation</p></th>
<th class="head"><p>Reference</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Precision</p></td>
<td><p><span class="math notranslate nohighlight">\(\geq 0\)</span> and <span class="math notranslate nohighlight">\(\leq 1\)</span></p></td>
<td><p>The closer to <span class="math notranslate nohighlight">\(1\)</span> the better.</p></td>
<td><p>Only for hits in recommendations.</p></td>
<td><p><a class="reference external" href="https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html#ranking-systems">link</a></p></td>
</tr>
<tr class="row-odd"><td><p>Recall</p></td>
<td><p><span class="math notranslate nohighlight">\(\geq 0\)</span> and <span class="math notranslate nohighlight">\(\leq 1\)</span></p></td>
<td><p>The closer to <span class="math notranslate nohighlight">\(1\)</span> the better.</p></td>
<td><p>Only for hits in the ground truth.</p></td>
<td><p><a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">link</a></p></td>
</tr>
<tr class="row-even"><td><p>NDCG</p></td>
<td><p><span class="math notranslate nohighlight">\(\geq 0\)</span> and <span class="math notranslate nohighlight">\(\leq 1\)</span></p></td>
<td><p>The closer to <span class="math notranslate nohighlight">\(1\)</span> the better.</p></td>
<td><p>Does not penalize for bad/missing items, and does not perform for several equally good items.</p></td>
<td><p><a class="reference external" href="https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html#ranking-systems">link</a></p></td>
</tr>
<tr class="row-odd"><td><p>MAP</p></td>
<td><p><span class="math notranslate nohighlight">\(\geq 0\)</span> and <span class="math notranslate nohighlight">\(\leq 1\)</span></p></td>
<td><p>The closer to <span class="math notranslate nohighlight">\(1\)</span> the better.</p></td>
<td><p>Depend on variable distributions.</p></td>
<td><p><a class="reference external" href="https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html#ranking-systems">link</a></p></td>
</tr>
<tr class="row-even"><td><p>AUC</p></td>
<td><p><span class="math notranslate nohighlight">\(\geq 0\)</span> and <span class="math notranslate nohighlight">\(\leq 1\)</span></p></td>
<td><p>The closer to <span class="math notranslate nohighlight">\(1\)</span> the better. 0.5 indicates an uninformative classifier</p></td>
<td><p>Depend on the number of recommended items (k).</p></td>
<td><p><a class="reference external" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve">link</a></p></td>
</tr>
<tr class="row-odd"><td><p>Logloss</p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(\infty\)</span></p></td>
<td><p>The closer to <span class="math notranslate nohighlight">\(0\)</span> the better.</p></td>
<td><p>Logloss can be sensitive to imbalanced datasets.</p></td>
<td><p><a class="reference external" href="https://en.wikipedia.org/wiki/Cross_entropy#Relation_to_log-likelihood">link</a></p></td>
</tr>
</tbody>
</table>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># cleanup spark instance</span>
<span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Guy Shani and Asela Gunawardana, “Evaluating Recommendation Systems”, Recommender Systems Handbook, Springer, 2015.</p></li>
<li><p>PySpark MLlib evaluation metrics, url: <a class="reference external" href="https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html">https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html</a>.</p></li>
<li><p>Dimitris Paraschakis et al, “Comparative Evaluation of Top-N Recommenders in e-Commerce: An Industrial Perspective”, IEEE ICMLA, 2015, Miami, FL, USA.</p></li>
<li><p>Yehuda Koren and Robert Bell, “Advances in Collaborative Filtering”, Recommender Systems Handbook, Springer, 2015.</p></li>
<li><p>Chris Bishop, “Pattern Recognition and Machine Learning”, Springer, 2006.</p></li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "reco_pyspark"
        },
        kernelOptions: {
            kernelName: "reco_pyspark",
            path: "./examples/03_evaluate"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'reco_pyspark'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>
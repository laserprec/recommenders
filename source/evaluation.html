
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Evaluation module &#8212; &lt;h1 style=&#34;font-size:2em;text-align:center;color:#FF5733&#34;&gt;Recommenders&lt;/h1&gt;</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/tabs.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title"><h1 style="font-size:2em;text-align:center;color:#FF5733">Recommenders</h1></h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p class="caption" role="heading">
 <span class="caption-text">
  Getting Started Notebooks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../examples/00_quick_start/als_movielens.html">
   Running ALS on MovieLens (PySpark)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../examples/00_quick_start/ncf_movielens.html">
   Neural Collaborative Filtering on MovieLens dataset.
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/source/evaluation.rst"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.rst</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/microsoft/genalog"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/microsoft/genalog/issues/new?title=Issue%20on%20page%20%2Fsource/evaluation.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-recommenders.evaluation.python_evaluation">
   Python evaluation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-recommenders.evaluation.spark_evaluation">
   PySpark evaluation
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="evaluation-module">
<span id="evaluation"></span><h1>Evaluation module<a class="headerlink" href="#evaluation-module" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-recommenders.evaluation.python_evaluation">
<span id="python-evaluation"></span><h2>Python evaluation<a class="headerlink" href="#module-recommenders.evaluation.python_evaluation" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.auc">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">auc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#auc"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.auc" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the Area-Under-Curve metric for implicit feedback typed
recommender, where rating is binary and prediction is float number ranging
from 0 to 1.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve">https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve</a></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The evaluation does not require a leave-one-out scenario.
This metric does not calculate group-based AUC which considers the AUC scores
averaged across users. It is also not limited to k. Instead, it calculates the
scores on the entire prediction results regardless the users.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True data</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted data</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>auc_score (min=0, max=1)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.catalog_coverage">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">catalog_coverage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#catalog_coverage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.catalog_coverage" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate catalog coverage for recommendations across all users.
The metric definition is based on the “catalog coverage” definition in the following reference:</p>
<dl class="field-list simple">
<dt class="field-odd">Citation</dt>
<dd class="field-odd"><p>G. Shani and A. Gunawardana, Evaluating Recommendation Systems,
Recommender Systems Handbook pp. 257-297, 2010.</p>
</dd>
<dt class="field-even">Parameters</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they
have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.
Interaction here follows the <em>item choice model</em> from Castells et al.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item,
col_relevance (optional). Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
</ul>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>catalog coverage</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.distributional_coverage">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">distributional_coverage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#distributional_coverage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.distributional_coverage" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate distributional coverage for recommendations across all users.
The metric definition is based on formula (21) in the following reference:</p>
<dl class="field-list simple">
<dt class="field-odd">Citation</dt>
<dd class="field-odd"><p>G. Shani and A. Gunawardana, Evaluating Recommendation Systems,
Recommender Systems Handbook pp. 257-297, 2010.</p>
</dd>
<dt class="field-even">Parameters</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they
have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.
Interaction here follows the <em>item choice model</em> from Castells et al.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item,
col_relevance (optional). Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
</ul>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>distributional coverage</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.diversity">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">diversity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_feature_df</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_sim_measure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'item_cooccurrence_count'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'features'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_sim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sim'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_relevance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#diversity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.diversity" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate average diversity of recommendations across all users.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they have interacted with;
contains col_user, col_item. Assumed to not contain any duplicate rows.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item, col_relevance (optional).
Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>item_feature_df</strong> (<em>pandas.DataFrame</em>) – (Optional) It is required only when item_sim_measure=’item_feature_vector’.
It contains two columns: col_item and features (a feature vector).</p></li>
<li><p><strong>item_sim_measure</strong> (<em>str</em>) – (Optional) This column indicates which item similarity measure to be used.
Available measures include item_cooccurrence_count (default choice) and item_feature_vector.</p></li>
<li><p><strong>col_item_features</strong> (<em>str</em>) – item feature column name.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
<li><p><strong>col_sim</strong> (<em>str</em>) – This column indicates the column name for item similarity.</p></li>
<li><p><strong>col_relevance</strong> (<em>str</em>) – This column indicates whether the recommended item is actually relevant to the user or not.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>diversity.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.exp_var">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">exp_var</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#exp_var"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.exp_var" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate explained variance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True data. There should be no duplicate (userID, itemID) pairs</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted data. There should be no duplicate (userID, itemID) pairs</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Explained variance (min=0, max=1).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.get_top_k_items">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">get_top_k_items</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataframe</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#get_top_k_items"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.get_top_k_items" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the input customer-item-rating tuple in the format of Pandas
DataFrame, output a Pandas DataFrame in the dense format of top k items
for each user.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If it is implicit rating, just append a column of constants to be
ratings.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataframe</strong> (<em>pandas.DataFrame</em>) – DataFrame of rating data (in the format</p></li>
<li><p><strong>customerID-itemID-rating</strong><strong>)</strong> – </p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>k</strong> (<em>int</em><em> or </em><em>None</em>) – number of items for each user; None means that the input has already been</p></li>
<li><p><strong>again.</strong> (<em>filtered out top k items and sorted by ratings and there is no need to do that</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>DataFrame of top k items for each user, sorted by <cite>col_user</cite> and <cite>rank</cite></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>pandas.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.historical_item_novelty">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">historical_item_novelty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#historical_item_novelty"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.historical_item_novelty" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate novelty for each item. Novelty is computed as the minus logarithm of
(number of interactions with item / total number of interactions). The definition of the metric
is based on the following reference using the choice model (eqs. 1 and 6):</p>
<dl class="field-list simple">
<dt class="field-odd">Citation</dt>
<dd class="field-odd"><p>P. Castells, S. Vargas, and J. Wang, Novelty and diversity metrics for recommender systems:
choice, discovery and relevance, ECIR 2011</p>
</dd>
</dl>
<p>The novelty of an item can be defined relative to a set of observed events on the set of all items.
These can be events of user choice (item “is picked” by a random user) or user discovery
(item “is known” to a random user). The above definition of novelty reflects a factor of item popularity.
High novelty values correspond to long-tail items in the density function, that few users have interacted
with and low novelty values correspond to popular head items.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they
have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.
Interaction here follows the <em>item choice model</em> from Castells et al.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item,
col_relevance (optional). Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dataframe with the following columns: col_item, item_novelty.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>pandas.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.logloss">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">logloss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#logloss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.logloss" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the logloss metric for implicit feedback typed
recommender, where rating is binary and prediction is float number ranging
from 0 to 1.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Loss_functions_for_classification#Cross_entropy_loss_(Log_Loss">https://en.wikipedia.org/wiki/Loss_functions_for_classification#Cross_entropy_loss_(Log_Loss</a>)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True data</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted data</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>log_loss_score (min=-inf, max=inf)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.mae">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">mae</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#mae"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.mae" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate Mean Absolute Error.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True data. There should be no duplicate (userID, itemID) pairs</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted data. There should be no duplicate (userID, itemID) pairs</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Mean Absolute Error.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.map_at_k">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">map_at_k</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relevancy_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top_k'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#map_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.map_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean Average Precision at k</p>
<p>The implementation of MAP is referenced from Spark MLlib evaluation metrics.
<a class="reference external" href="https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html#ranking-systems">https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html#ranking-systems</a></p>
<p>A good reference can be found at:
<a class="reference external" href="http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf">http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf</a></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1. The evaluation function is named as ‘MAP is at k’ because the evaluation class takes top k items for
the prediction items. The naming is different from Spark.</p>
<p>2. The MAP is meant to calculate Avg. Precision for the relevant items, so it is normalized by the number of
relevant items in the ground truth data, instead of k.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True DataFrame</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted DataFrame</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
<li><p><strong>relevancy_method</strong> (<em>str</em>) – method for determining relevancy [‘top_k’, ‘by_threshold’, None]. None means that the
top k items are directly provided, so there is no need to compute the relevancy operation.</p></li>
<li><p><strong>k</strong> (<em>int</em>) – number of top k items per user</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – threshold of top items per user (optional)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>MAP at k (min=0, max=1).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.merge_ranking_true_pred">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">merge_ranking_true_pred</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relevancy_method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#merge_ranking_true_pred"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.merge_ranking_true_pred" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter truth and prediction data frames on common users</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True DataFrame</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted DataFrame</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
<li><p><strong>relevancy_method</strong> (<em>str</em>) – method for determining relevancy [‘top_k’, ‘by_threshold’, None]. None means that the
top k items are directly provided, so there is no need to compute the relevancy operation.</p></li>
<li><p><strong>k</strong> (<em>int</em>) – number of top k items per user (optional)</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – threshold of top items per user (optional)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>DataFrame of recommendation hits, sorted by <cite>col_user</cite> and <cite>rank</cite>
DataFrame of hit counts vs actual relevant items per user number of unique user ids</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>pandas.DataFrame, pandas.DataFrame, int</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.merge_rating_true_pred">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">merge_rating_true_pred</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#merge_rating_true_pred"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.merge_rating_true_pred" title="Permalink to this definition">¶</a></dt>
<dd><p>Join truth and prediction data frames on userID and itemID and return the true
and predicted rated with the correct index.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True data</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted data</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Array with the true ratings
numpy.ndarray: Array with the predicted ratings</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.ndcg_at_k">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">ndcg_at_k</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relevancy_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top_k'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#ndcg_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.ndcg_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Normalized Discounted Cumulative Gain (nDCG).</p>
<p>Info: <a class="reference external" href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain">https://en.wikipedia.org/wiki/Discounted_cumulative_gain</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True DataFrame</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted DataFrame</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
<li><p><strong>relevancy_method</strong> (<em>str</em>) – method for determining relevancy [‘top_k’, ‘by_threshold’, None]. None means that the
top k items are directly provided, so there is no need to compute the relevancy operation.</p></li>
<li><p><strong>k</strong> (<em>int</em>) – number of top k items per user</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – threshold of top items per user (optional)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>nDCG at k (min=0, max=1).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.novelty">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">novelty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#novelty"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.novelty" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the average novelty in a list of recommended items (this assumes that the recommendation list
is already computed). Follows section 5 from</p>
<dl class="field-list simple">
<dt class="field-odd">Citation</dt>
<dd class="field-odd"><p>P. Castells, S. Vargas, and J. Wang, Novelty and diversity metrics for recommender systems:
choice, discovery and relevance, ECIR 2011</p>
</dd>
<dt class="field-even">Parameters</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they
have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.
Interaction here follows the <em>item choice model</em> from Castells et al.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item,
col_relevance (optional). Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
</ul>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>novelty.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.precision_at_k">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">precision_at_k</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relevancy_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top_k'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#precision_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.precision_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Precision at K.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We use the same formula to calculate precision&#64;k as that in Spark.
More details can be found at
<a class="reference external" href="http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.precisionAt">http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.precisionAt</a>
In particular, the maximum achievable precision may be &lt; 1, if the number of items for a
user in rating_pred is less than k.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True DataFrame</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted DataFrame</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
<li><p><strong>relevancy_method</strong> (<em>str</em>) – method for determining relevancy [‘top_k’, ‘by_threshold’, None]. None means that the
top k items are directly provided, so there is no need to compute the relevancy operation.</p></li>
<li><p><strong>k</strong> (<em>int</em>) – number of top k items per user</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – threshold of top items per user (optional)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>precision at k (min=0, max=1)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.recall_at_k">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">recall_at_k</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relevancy_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top_k'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#recall_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.recall_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Recall at K.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True DataFrame</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted DataFrame</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
<li><p><strong>relevancy_method</strong> (<em>str</em>) – method for determining relevancy [‘top_k’, ‘by_threshold’, None]. None means that the
top k items are directly provided, so there is no need to compute the relevancy operation.</p></li>
<li><p><strong>k</strong> (<em>int</em>) – number of top k items per user</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – threshold of top items per user (optional)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>recall at k (min=0, max=1). The maximum value is 1 even when fewer than
k items exist for a user in rating_true.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.rmse">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">rmse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#rmse"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.rmse" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate Root Mean Squared Error</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True data. There should be no duplicate (userID, itemID) pairs</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted data. There should be no duplicate (userID, itemID) pairs</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Root mean squared error</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.rsquared">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">rsquared</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#rsquared"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.rsquared" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate R squared</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pandas.DataFrame</em>) – True data. There should be no duplicate (userID, itemID) pairs</p></li>
<li><p><strong>rating_pred</strong> (<em>pandas.DataFrame</em>) – Predicted data. There should be no duplicate (userID, itemID) pairs</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>R squared (min=0, max=1).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.serendipity">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">serendipity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_feature_df</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_sim_measure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'item_cooccurrence_count'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'features'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_sim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sim'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_relevance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#serendipity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.serendipity" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate average serendipity for recommendations across all users.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they
have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item,
col_relevance (optional). Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>item_feature_df</strong> (<em>pandas.DataFrame</em>) – (Optional) It is required only when item_sim_measure=’item_feature_vector’.
It contains two columns: col_item and features (a feature vector).</p></li>
<li><p><strong>item_sim_measure</strong> (<em>str</em>) – (Optional) This column indicates which item similarity measure to be used.
Available measures include item_cooccurrence_count (default choice) and item_feature_vector.</p></li>
<li><p><strong>col_item_features</strong> (<em>str</em>) – item feature column name.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
<li><p><strong>col_sim</strong> (<em>str</em>) – This column indicates the column name for item similarity.</p></li>
<li><p><strong>col_relevance</strong> (<em>str</em>) – This column indicates whether the recommended item is actually
relevant to the user or not.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>serendipity.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.user_diversity">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">user_diversity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_feature_df</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_sim_measure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'item_cooccurrence_count'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'features'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_sim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sim'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_relevance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#user_diversity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.user_diversity" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate average diversity of recommendations for each user.
The metric definition is based on formula (3) in the following reference:</p>
<dl class="field-list simple">
<dt class="field-odd">Citation</dt>
<dd class="field-odd"><p>Y.C. Zhang, D.Ó. Séaghdha, D. Quercia and T. Jambor, Auralist:
introducing serendipity into music recommendation, WSDM 2012</p>
</dd>
<dt class="field-even">Parameters</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they have interacted with;
contains col_user, col_item. Assumed to not contain any duplicate rows.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item, col_relevance (optional).
Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>item_feature_df</strong> (<em>pandas.DataFrame</em>) – (Optional) It is required only when item_sim_measure=’item_feature_vector’.
It contains two columns: col_item and features (a feature vector).</p></li>
<li><p><strong>item_sim_measure</strong> (<em>str</em>) – (Optional) This column indicates which item similarity measure to be used.
Available measures include item_cooccurrence_count (default choice) and item_feature_vector.</p></li>
<li><p><strong>col_item_features</strong> (<em>str</em>) – item feature column name.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
<li><p><strong>col_sim</strong> (<em>str</em>) – This column indicates the column name for item similarity.</p></li>
<li><p><strong>col_relevance</strong> (<em>str</em>) – This column indicates whether the recommended item is actually relevant to the user or not.</p></li>
</ul>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A dataframe with the following columns: col_user, user_diversity.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>pandas.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.user_item_serendipity">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">user_item_serendipity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_feature_df</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_sim_measure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'item_cooccurrence_count'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'features'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_sim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sim'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_relevance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#user_item_serendipity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.user_item_serendipity" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate serendipity of each item in the recommendations for each user.
The metric definition is based on the following references:</p>
<dl class="field-list simple">
<dt class="field-odd">Citation</dt>
<dd class="field-odd"><p></p></dd>
</dl>
<p>Y.C. Zhang, D.Ó. Séaghdha, D. Quercia and T. Jambor, Auralist:
introducing serendipity into music recommendation, WSDM 2012</p>
<p>Eugene Yan, Serendipity: Accuracy’s unpopular best friend in Recommender Systems,
eugeneyan.com, April 2020</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they
have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item,
col_relevance (optional). Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>item_feature_df</strong> (<em>pandas.DataFrame</em>) – (Optional) It is required only when item_sim_measure=’item_feature_vector’.
It contains two columns: col_item and features (a feature vector).</p></li>
<li><p><strong>item_sim_measure</strong> (<em>str</em>) – (Optional) This column indicates which item similarity measure to be used.
Available measures include item_cooccurrence_count (default choice) and item_feature_vector.</p></li>
<li><p><strong>col_item_features</strong> (<em>str</em>) – item feature column name.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
<li><p><strong>col_sim</strong> (<em>str</em>) – This column indicates the column name for item similarity.</p></li>
<li><p><strong>col_relevance</strong> (<em>str</em>) – This column indicates whether the recommended item is actually
relevant to the user or not.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dataframe with columns: col_user, col_item, user_item_serendipity.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>pandas.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="recommenders.evaluation.python_evaluation.user_serendipity">
<span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.python_evaluation.</span></span><span class="sig-name descname"><span class="pre">user_serendipity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_feature_df</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_sim_measure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'item_cooccurrence_count'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'features'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_sim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sim'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_relevance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/python_evaluation.html#user_serendipity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.python_evaluation.user_serendipity" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate average serendipity for each user’s recommendations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pandas.DataFrame</em>) – Data set with historical data for users and items they
have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.</p></li>
<li><p><strong>reco_df</strong> (<em>pandas.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item,
col_relevance (optional). Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>item_feature_df</strong> (<em>pandas.DataFrame</em>) – (Optional) It is required only when item_sim_measure=’item_feature_vector’.
It contains two columns: col_item and features (a feature vector).</p></li>
<li><p><strong>item_sim_measure</strong> (<em>str</em>) – (Optional) This column indicates which item similarity measure to be used.
Available measures include item_cooccurrence_count (default choice) and item_feature_vector.</p></li>
<li><p><strong>col_item_features</strong> (<em>str</em>) – item feature column name.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
<li><p><strong>col_sim</strong> (<em>str</em>) – This column indicates the column name for item similarity.</p></li>
<li><p><strong>col_relevance</strong> (<em>str</em>) – This column indicates whether the recommended item is actually
relevant to the user or not.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dataframe with following columns: col_user, user_serendipity.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>pandas.DataFrame</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-recommenders.evaluation.spark_evaluation">
<span id="pyspark-evaluation"></span><h2>PySpark evaluation<a class="headerlink" href="#module-recommenders.evaluation.spark_evaluation" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.spark_evaluation.</span></span><span class="sig-name descname"><span class="pre">SparkDiversityEvaluation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reco_df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_feature_df</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_sim_measure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'item_cooccurrence_count'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_relevance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation" title="Permalink to this definition">¶</a></dt>
<dd><p>Spark Evaluator for diversity, coverage, novelty, serendipity</p>
<p>Initializer.</p>
<p>This is the Spark version of diversity metrics evaluator.
The methods of this class calculate the following diversity metrics:</p>
<ul class="simple">
<li><dl class="simple">
<dt>Coverage - it includes two metrics:</dt><dd><ol class="arabic simple">
<li><p>catalog_coverage, which measures the proportion of items that get recommended from the item catalog;</p></li>
<li><p>distributional_coverage, which measures how unequally different items are recommended in the
recommendations to all users.</p></li>
</ol>
</dd>
</dl>
</li>
<li><p>Novelty - A more novel item indicates it is less popular, i.e. it gets recommended less frequently.</p></li>
<li><p>Diversity - The dissimilarity of items being recommended.</p></li>
<li><dl class="simple">
<dt>Serendipity - The “unusualness” or “surprise” of recommendations to a user. When ‘col_relevance’ is used,</dt><dd><p>it indicates how “pleasant surprise” of recommendations is to a user.</p>
</dd>
</dl>
</li>
</ul>
<p>The metric definitions/formulations are based on the following references with modification:</p>
<dl class="field-list">
<dt class="field-odd">Citation</dt>
<dd class="field-odd"><p>G. Shani and A. Gunawardana, Evaluating Recommendation Systems,
Recommender Systems Handbook pp. 257-297, 2010.</p>
<p>Y.C. Zhang, D.Ó. Séaghdha, D. Quercia and T. Jambor, Auralist: introducing
serendipity into music recommendation, WSDM 2012</p>
<p>P. Castells, S. Vargas, and J. Wang, Novelty and diversity metrics for recommender systems:
choice, discovery and relevance, ECIR 2011</p>
<p>Eugene Yan, Serendipity: Accuracy’s unpopular best friend in Recommender Systems,
eugeneyan.com, April 2020</p>
</dd>
<dt class="field-even">Parameters</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>train_df</strong> (<em>pyspark.sql.DataFrame</em>) – Data set with historical data for users and items they
have interacted with; contains col_user, col_item. Assumed to not contain any duplicate rows.
Interaction here follows the <em>item choice model</em> from Castells et al.</p></li>
<li><p><strong>reco_df</strong> (<em>pyspark.sql.DataFrame</em>) – Recommender’s prediction output, containing col_user, col_item,
col_relevance (optional). Assumed to not contain any duplicate user-item pairs.</p></li>
<li><p><strong>item_feature_df</strong> (<em>pyspark.sql.DataFrame</em>) – (Optional) It is required only when item_sim_measure=’item_feature_vector’.
It contains two columns: col_item and features (a feature vector).</p></li>
<li><p><strong>item_sim_measure</strong> (<em>str</em>) – (Optional) This column indicates which item similarity measure to be used.
Available measures include item_cooccurrence_count (default choice) and item_feature_vector.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – User id column name.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – Item id column name.</p></li>
<li><p><strong>col_relevance</strong> (<em>str</em>) – Optional. This column indicates whether the recommended item is actually
relevant to the user or not.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.catalog_coverage">
<span class="sig-name descname"><span class="pre">catalog_coverage</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.catalog_coverage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.catalog_coverage" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate catalog coverage for recommendations across all users.
The metric definition is based on the “catalog coverage” definition in the following reference:</p>
<dl class="field-list simple">
<dt class="field-odd">Citation</dt>
<dd class="field-odd"><p>G. Shani and A. Gunawardana, Evaluating Recommendation Systems,
Recommender Systems Handbook pp. 257-297, 2010.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>catalog coverage</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.distributional_coverage">
<span class="sig-name descname"><span class="pre">distributional_coverage</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.distributional_coverage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.distributional_coverage" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate distributional coverage for recommendations across all users.
The metric definition is based on formula (21) in the following reference:</p>
<dl class="field-list simple">
<dt class="field-odd">Citation</dt>
<dd class="field-odd"><p>G. Shani and A. Gunawardana, Evaluating Recommendation Systems,
Recommender Systems Handbook pp. 257-297, 2010.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>distributional coverage</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.diversity">
<span class="sig-name descname"><span class="pre">diversity</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.diversity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.diversity" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate average diversity of recommendations across all users.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>diversity.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.historical_item_novelty">
<span class="sig-name descname"><span class="pre">historical_item_novelty</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.historical_item_novelty"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.historical_item_novelty" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate novelty for each item. Novelty is computed as the minus logarithm of
(number of interactions with item / total number of interactions). The definition of the metric
is based on the following reference using the choice model (eqs. 1 and 6):</p>
<dl class="field-list simple">
<dt class="field-odd">Citation</dt>
<dd class="field-odd"><p>P. Castells, S. Vargas, and J. Wang, Novelty and diversity metrics for recommender systems:
choice, discovery and relevance, ECIR 2011</p>
</dd>
</dl>
<p>The novelty of an item can be defined relative to a set of observed events on the set of all items.
These can be events of user choice (item “is picked” by a random user) or user discovery
(item “is known” to a random user). The above definition of novelty reflects a factor of item popularity.
High novelty values correspond to long-tail items in the density function, that few users have interacted
with and low novelty values correspond to popular head items.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A dataframe with the following columns: col_item, item_novelty.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>pyspark.sql.dataframe.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.novelty">
<span class="sig-name descname"><span class="pre">novelty</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.novelty"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.novelty" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the average novelty in a list of recommended items (this assumes that the recommendation list
is already computed). Follows section 5 from</p>
<dl class="field-list simple">
<dt class="field-odd">Citation</dt>
<dd class="field-odd"><p>P. Castells, S. Vargas, and J. Wang, Novelty and diversity metrics for recommender systems:
choice, discovery and relevance, ECIR 2011</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dataframe with following columns: novelty.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>pyspark.sql.dataframe.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.serendipity">
<span class="sig-name descname"><span class="pre">serendipity</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.serendipity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.serendipity" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate average serendipity for recommendations across all users.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>serendipity.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_diversity">
<span class="sig-name descname"><span class="pre">user_diversity</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.user_diversity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_diversity" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate average diversity of recommendations for each user.
The metric definition is based on formula (3) in the following reference:</p>
<dl class="field-list simple">
<dt class="field-odd">Citation</dt>
<dd class="field-odd"><p>Y.C. Zhang, D.Ó. Séaghdha, D. Quercia and T. Jambor, Auralist:
introducing serendipity into music recommendation, WSDM 2012</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dataframe with the following columns: col_user, user_diversity.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>pyspark.sql.dataframe.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_item_serendipity">
<span class="sig-name descname"><span class="pre">user_item_serendipity</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.user_item_serendipity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_item_serendipity" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate serendipity of each item in the recommendations for each user.
The metric definition is based on the following references:</p>
<dl class="field-list">
<dt class="field-odd">Citation</dt>
<dd class="field-odd"><p>Y.C. Zhang, D.Ó. Séaghdha, D. Quercia and T. Jambor, Auralist:
introducing serendipity into music recommendation, WSDM 2012</p>
<p>Eugene Yan, Serendipity: Accuracy’s unpopular best friend in Recommender Systems,
eugeneyan.com, April 2020</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dataframe with columns: col_user, col_item, user_item_serendipity.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>pyspark.sql.dataframe.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_serendipity">
<span class="sig-name descname"><span class="pre">user_serendipity</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkDiversityEvaluation.user_serendipity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkDiversityEvaluation.user_serendipity" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate average serendipity for each user’s recommendations.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A dataframe with following columns: col_user, user_serendipity.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>pyspark.sql.dataframe.DataFrame</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRankingEvaluation">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.spark_evaluation.</span></span><span class="sig-name descname"><span class="pre">SparkRankingEvaluation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relevancy_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top_k'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkRankingEvaluation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation" title="Permalink to this definition">¶</a></dt>
<dd><p>Spark Ranking Evaluator</p>
<p>Initialization.
This is the Spark version of ranking metrics evaluator.
The methods of this class, calculate ranking metrics such as precision&#64;k, recall&#64;k, ndcg&#64;k, and mean average
precision.</p>
<p>The implementations of precision&#64;k, ndcg&#64;k, and mean average precision are referenced from Spark MLlib, which
can be found at <a class="reference external" href="https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html#ranking-systems">here</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pyspark.sql.DataFrame</em>) – DataFrame of true rating data (in the
format of customerID-itemID-rating tuple).</p></li>
<li><p><strong>rating_pred</strong> (<em>pyspark.sql.DataFrame</em>) – DataFrame of predicted rating data (in
the format of customerID-itemID-rating tuple).</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item.</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating.</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction.</p></li>
<li><p><strong>k</strong> (<em>int</em>) – number of items to recommend to each user.</p></li>
<li><p><strong>relevancy_method</strong> (<em>str</em>) – method for determining relevant items. Possible
values are “top_k”, “by_time_stamp”, and “by_threshold”.</p></li>
<li><p><strong>threshold</strong> (<em>float</em>) – threshold for determining the relevant recommended items.
This is used for the case that predicted ratings follow a known
distribution. NOTE: this option is only activated if relevancy_method is
set to “by_threshold”.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.map_at_k">
<span class="sig-name descname"><span class="pre">map_at_k</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkRankingEvaluation.map_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.map_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Get mean average precision at k.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>More details can be found
<a class="reference external" href="http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.meanAveragePrecision">here</a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>MAP at k (min=0, max=1).</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.ndcg_at_k">
<span class="sig-name descname"><span class="pre">ndcg_at_k</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkRankingEvaluation.ndcg_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.ndcg_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Get Normalized Discounted Cumulative Gain (NDCG)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>More details can be found
<a class="reference external" href="http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.ndcgAt">here</a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>nDCG at k (min=0, max=1).</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.precision_at_k">
<span class="sig-name descname"><span class="pre">precision_at_k</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkRankingEvaluation.precision_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.precision_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Get precision&#64;k.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>More details can be found
<a class="reference external" href="http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.precisionAt">here</a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>precision at k (min=0, max=1)</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.recall_at_k">
<span class="sig-name descname"><span class="pre">recall_at_k</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkRankingEvaluation.recall_at_k"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRankingEvaluation.recall_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Get recall&#64;K.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>More details can be found
<a class="reference external" href="http://spark.apache.org/docs/2.1.1/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.meanAveragePrecision">here</a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>recall at k (min=0, max=1).</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRatingEvaluation">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">recommenders.evaluation.spark_evaluation.</span></span><span class="sig-name descname"><span class="pre">SparkRatingEvaluation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rating_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rating_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_user</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'userID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'itemID'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_rating</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rating'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">col_prediction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'prediction'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkRatingEvaluation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation" title="Permalink to this definition">¶</a></dt>
<dd><p>Spark Rating Evaluator</p>
<p>Initializer.</p>
<p>This is the Spark version of rating metrics evaluator.
The methods of this class, calculate rating metrics such as root mean squared error, mean absolute error,
R squared, and explained variance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rating_true</strong> (<em>pyspark.sql.DataFrame</em>) – True labels.</p></li>
<li><p><strong>rating_pred</strong> (<em>pyspark.sql.DataFrame</em>) – Predicted labels.</p></li>
<li><p><strong>col_user</strong> (<em>str</em>) – column name for user.</p></li>
<li><p><strong>col_item</strong> (<em>str</em>) – column name for item.</p></li>
<li><p><strong>col_rating</strong> (<em>str</em>) – column name for rating.</p></li>
<li><p><strong>col_prediction</strong> (<em>str</em>) – column name for prediction.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.exp_var">
<span class="sig-name descname"><span class="pre">exp_var</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkRatingEvaluation.exp_var"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.exp_var" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate explained variance.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Spark MLLib’s implementation is buggy (can lead to values &gt; 1), hence we use var().</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Explained variance (min=0, max=1).</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.mae">
<span class="sig-name descname"><span class="pre">mae</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkRatingEvaluation.mae"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.mae" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate Mean Absolute Error.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Mean Absolute Error.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.rmse">
<span class="sig-name descname"><span class="pre">rmse</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkRatingEvaluation.rmse"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.rmse" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate Root Mean Squared Error.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Root mean squared error.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.rsquared">
<span class="sig-name descname"><span class="pre">rsquared</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/recommenders/evaluation/spark_evaluation.html#SparkRatingEvaluation.rsquared"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#recommenders.evaluation.spark_evaluation.SparkRatingEvaluation.rsquared" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate R squared.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>R squared.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./source"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>